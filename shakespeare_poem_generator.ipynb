{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWZNQQxqqQWG"
      },
      "source": [
        "## Deep Learning Mini project - Shakespeare poem generator\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxhS-D4o8r-I"
      },
      "source": [
        "### Summary\n",
        "\n",
        "- Eventually I ended up using pretrained GPT2 embeddings, since it greatly enhances the training without the need of a robust model and huge resources.\n",
        "- Hybrid loss: I utilized both Cross-Entropy and Focal loss by combining them. Focal loss proved to be very helpful by focusing more on rare words in the vocabulary.\n",
        "- Bleu score: This metric served as a guidance on the reliability of the evaluation, but it is not a strict condition in the interpretation, so some other metric could be better in future trials.\n",
        "- Tested different parameter settings in the poem generation part."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZmuP5wzUTd0"
      },
      "source": [
        "### Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-29T09:31:51.594641Z",
          "iopub.status.busy": "2025-04-29T09:31:51.594293Z",
          "iopub.status.idle": "2025-04-29T09:31:51.599563Z",
          "shell.execute_reply": "2025-04-29T09:31:51.598708Z",
          "shell.execute_reply.started": "2025-04-29T09:31:51.594613Z"
        },
        "id": "Frk0rC90Ud0X"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import gc\n",
        "import spacy\n",
        "import kornia\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from transformers import GPT2Model, GPT2Tokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from datasets import Dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-29T09:31:51.600751Z",
          "iopub.status.busy": "2025-04-29T09:31:51.600552Z",
          "iopub.status.idle": "2025-04-29T09:31:51.614367Z",
          "shell.execute_reply": "2025-04-29T09:31:51.613731Z",
          "shell.execute_reply.started": "2025-04-29T09:31:51.600733Z"
        },
        "id": "m4zOls4H8r-I"
      },
      "outputs": [],
      "source": [
        "# Settings for ideal prints\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', 999)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-29T09:31:51.615798Z",
          "iopub.status.busy": "2025-04-29T09:31:51.615599Z",
          "iopub.status.idle": "2025-04-29T09:31:51.628796Z",
          "shell.execute_reply": "2025-04-29T09:31:51.628084Z",
          "shell.execute_reply.started": "2025-04-29T09:31:51.615780Z"
        },
        "id": "W1aCd7iwUXCJ"
      },
      "outputs": [],
      "source": [
        "class config:\n",
        "    # dataset params\n",
        "    lines = 3 # number of LINEBREAK separated sentence to include in a sample (LSTM remembers short seqs so 3-4 is max)\n",
        "    max_length = 128 # max number of tokens per sample\n",
        "    min_words_per_sample = 30 # min num of words per sample\n",
        "    max_words_per_sample = 100 # max num of words per sample\n",
        "\n",
        "    # train params\n",
        "    batch_size = 16\n",
        "    epochs = 20\n",
        "    lr = 1e-2\n",
        "    wd = 1e-4\n",
        "    early_stopping = 4\n",
        "\n",
        "    # model params\n",
        "    pretrained_emb = True # apply pretrained embeddings or train it from zero\n",
        "    embedding_dim = 100 # embedding dimension size (opt: 64-100-128 use std 100-200 etc if glove)\n",
        "    lstm_hidden_dim = 128 # size of lstm hidden neurons (opt: 128-256)\n",
        "    num_layers = 2 # number of lstm layers (opt: 1-2)?\n",
        "    dropout = 0.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VySq5ngNMAgL"
      },
      "source": [
        "### 1. Prepare dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-29T09:31:51.630226Z",
          "iopub.status.busy": "2025-04-29T09:31:51.629897Z",
          "iopub.status.idle": "2025-04-29T09:31:51.917434Z",
          "shell.execute_reply": "2025-04-29T09:31:51.916619Z",
          "shell.execute_reply.started": "2025-04-29T09:31:51.630197Z"
        },
        "id": "vW1MjHrovQn6",
        "outputId": "9c1953d4-cd86-4b04-a9df-65dc63415903"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-04-29 09:31:51--  https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt\n",
            "Resolving ocw.mit.edu (ocw.mit.edu)... 151.101.66.133, 151.101.2.133, 151.101.194.133, ...\n",
            "Connecting to ocw.mit.edu (ocw.mit.edu)|151.101.66.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5458199 (5.2M) [text/plain]\n",
            "Saving to: ‘t8.shakespeare.txt.1’\n",
            "\n",
            "t8.shakespeare.txt. 100%[===================>]   5.21M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2025-04-29 09:31:51 (116 MB/s) - ‘t8.shakespeare.txt.1’ saved [5458199/5458199]\n",
            "\n",
            "/content/sample_data: Scheme missing.\n",
            "FINISHED --2025-04-29 09:31:51--\n",
            "Total wall clock time: 0.1s\n",
            "Downloaded: 1 files, 5.2M in 0.04s (116 MB/s)\n"
          ]
        }
      ],
      "source": [
        "# Download the dataset\n",
        "!wget https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt /content/sample_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-29T09:31:51.918791Z",
          "iopub.status.busy": "2025-04-29T09:31:51.918491Z",
          "iopub.status.idle": "2025-04-29T09:31:51.934972Z",
          "shell.execute_reply": "2025-04-29T09:31:51.934354Z",
          "shell.execute_reply.started": "2025-04-29T09:31:51.918766Z"
        },
        "id": "xMCvTphyurs4",
        "outputId": "f44c47c5-336c-4494-ef14-814f6c140fdf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\" From fairest creatures we desire increase,\\n  That thereby beauty's rose might never die,\\n  But as the riper should by time decease,\\n  His tender heir might bear his memory:\\n  But thou contracted to thine own bright eyes,\\n  Feed'st thy light's flame with self-substantial fuel,\\n  Making a famine where abundance lies,\\n  Thy self thy foe, to thy sweet self too cruel:\\n  Thou that art now the world's fresh ornament,\\n  And only herald to the gaudy spring,\\n  Within thine own bu\""
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "with open(\"t8.shakespeare.txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "    raw_text = file.read()\n",
        "\n",
        "raw_text[10525:11000] # content starts around here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-29T09:31:51.936758Z",
          "iopub.status.busy": "2025-04-29T09:31:51.936551Z",
          "iopub.status.idle": "2025-04-29T09:31:53.475296Z",
          "shell.execute_reply": "2025-04-29T09:31:53.474428Z",
          "shell.execute_reply.started": "2025-04-29T09:31:51.936739Z"
        },
        "id": "3YDEpu-w4uSc",
        "outputId": "ae61ac09-7fef-4d0e-8d49-f4336392a9f3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"from fairest creatures we desire increase, [LINEBREAK] that thereby beauty's rose might never die, [LINEBREAK] but as the riper should by time decease, [LINEBREAK] his tender heir might bear his memory: [LINEBREAK] but thou contracted to thine own bright eyes, [LINEBREAK] feed'st thy light's flame with self-substantial fuel, [LINEBREAK] making a famine where abundance lies, [LINEBREAK] thy self thy foe, to thy sweet self too cruel: [LINEBREAK] thou that art now the world's fresh ornament, [LINEBREAK] and only herald to the gaudy spring, [LINEBREAK] within thine own bud buriest thy content, [LINEBREAK] and tender churl mak'st waste in niggarding: [LINEBREAK] pity the world, or else this glutton be, [LINEBREAK] to eat the world's due, by the grave and thee. [STANZABREAK] 2 [LINEBREAK] when forty winters shall besiege thy brow, [LINEBREAK] and dig deep trenches in thy beauty's field, [LINEBREAK] thy youth's proud livery so gazed on now, [LINEBREAK] will be a tattered weed of small worth h\""
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def preserve_names(match):\n",
        "    \"\"\" Lowercase everything except character names. \"\"\"\n",
        "    word = match.group(0)\n",
        "    return word if word.isupper() else word.lower()\n",
        "\n",
        "def reformat_text(text):\n",
        "    # Remove Gutenberg metadata, play directions, and formatting artifacts\n",
        "    text = re.sub(r'(?s)\\*{3,} END OF (THE|THIS) PROJECT GUTENBERG E?TEXT.*', '', text)\n",
        "    text = re.sub(r'\\b(ACT|SCENE|Enter|Exit|Exeunt|Re-enter|SC_\\d+)\\b', '', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'<<[^>]*>>|\\*[^*]*\\*|\\[[^\\]]*\\]', '', text)  # remove <<...>>, *...*, and [ ... ]\n",
        "    # Convert multi-line breaks to stanza/token breaks\n",
        "    text = text.replace('\\r', '')  # clean carriage returns\n",
        "    text = re.sub(r'\\n{2,}', ' [STANZABREAK] ', text) # indicator of 2+ newline\n",
        "    text = re.sub(r'\\n', ' [LINEBREAK] ', text) # indicator of newline\n",
        "    # # # Normalize spacing around punctuation\n",
        "    # text = re.sub(r'\\s*([.,:;!?])\\s*', r' \\1 ', text)\n",
        "    # Lowercase everything except full uppercase names\n",
        "    text = re.sub(r'\\b\\w+\\b', preserve_names, text)\n",
        "    # Collapse multiple spaces into one\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text.strip()\n",
        "\n",
        "cleaned_text = reformat_text(raw_text[10525:])\n",
        "cleaned_text[:1000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-29T09:31:53.477026Z",
          "iopub.status.busy": "2025-04-29T09:31:53.476783Z",
          "iopub.status.idle": "2025-04-29T09:31:53.654924Z",
          "shell.execute_reply": "2025-04-29T09:31:53.653979Z",
          "shell.execute_reply.started": "2025-04-29T09:31:53.477004Z"
        },
        "id": "dCZxYgA58r-I",
        "outputId": "f898675b-4ef3-44bb-cd0b-7916abf93e3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of lines: 105848\n",
            "Avg number of words per lines: 9.28\n",
            "Max number of words per lines: 42\n"
          ]
        }
      ],
      "source": [
        "# Split the text by indicator\n",
        "# - LINEBREAK: num of lines: 105848; avg words: 9.28   max words: 42\n",
        "# - STANZABREAK: num of \"lines\": 6197; avg words: 158.67 max words: 4015\n",
        "# It is probably better to go with linebreaks, since there are huge blocks in the stanza version\n",
        "\n",
        "raw_lines = cleaned_text.split('[LINEBREAK]')\n",
        "raw_lines = [line.strip()+' [LINEBREAK]' for line in raw_lines if line.strip()]\n",
        "avg_line_len = np.mean([len(line.split()) for line in raw_lines])\n",
        "max_line_len = np.max([len(line.split()) for line in raw_lines])\n",
        "print(f'Number of lines: {len(raw_lines)}')\n",
        "print(f\"Avg number of words per lines: {avg_line_len:.2f}\")\n",
        "print(f\"Max number of words per lines: {max_line_len}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-29T09:31:53.656236Z",
          "iopub.status.busy": "2025-04-29T09:31:53.655920Z",
          "iopub.status.idle": "2025-04-29T09:31:53.661493Z",
          "shell.execute_reply": "2025-04-29T09:31:53.660706Z",
          "shell.execute_reply.started": "2025-04-29T09:31:53.656203Z"
        },
        "id": "oBRQBb808r-I",
        "outputId": "4c5361c2-2bba-45ec-d5e4-b67207704d7a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[\"why lov'st thou that which thou receiv'st not gladly, [LINEBREAK]\",\n",
              " \"or else receiv'st with pleasure thine annoy? [LINEBREAK]\",\n",
              " 'if the true concord of well-tuned sounds, [LINEBREAK]',\n",
              " 'by unions married do offend thine ear, [LINEBREAK]',\n",
              " 'they do but sweetly chide thee, who confounds [LINEBREAK]',\n",
              " 'in singleness the parts that thou shouldst bear: [LINEBREAK]',\n",
              " 'mark how one string sweet husband to another, [LINEBREAK]',\n",
              " 'strikes each in each by mutual ordering; [LINEBREAK]',\n",
              " 'resembling sire, and child, and happy mother, [LINEBREAK]',\n",
              " 'who all in one, one pleasing note do sing: [LINEBREAK]']"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "raw_lines[100:110]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6FgM1hlRUCL"
      },
      "source": [
        "### 2. Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-29T09:31:53.662564Z",
          "iopub.status.busy": "2025-04-29T09:31:53.662299Z",
          "iopub.status.idle": "2025-04-29T09:31:53.901310Z",
          "shell.execute_reply": "2025-04-29T09:31:53.900590Z",
          "shell.execute_reply.started": "2025-04-29T09:31:53.662544Z"
        },
        "id": "A-ARYq-mTV_j",
        "outputId": "ed92eeb0-ed6d-4c0c-9346-c82c05eee1a8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token  # GPT2 uses eos as pad\n",
        "tokenizer.add_special_tokens({'additional_special_tokens': ['[LINEBREAK]', '[STANZABREAK]']})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-29T09:31:53.902378Z",
          "iopub.status.busy": "2025-04-29T09:31:53.902122Z",
          "iopub.status.idle": "2025-04-29T09:31:53.912156Z",
          "shell.execute_reply": "2025-04-29T09:31:53.911479Z",
          "shell.execute_reply.started": "2025-04-29T09:31:53.902357Z"
        },
        "id": "AiY40yTS8r-I"
      },
      "outputs": [],
      "source": [
        "class GPT2TokenDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, lines, tokenizer, lines_per_sample=3, max_length=128, min_words_per_sample=30, max_words_per_sample=100, debug_samples=False):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.lines_per_sample = lines_per_sample\n",
        "        self.max_length = max_length\n",
        "        self.min_words_per_sample = min_words_per_sample\n",
        "        self.max_words_per_sample = max_words_per_sample\n",
        "        self.debug_samples = debug_samples\n",
        "        self.samples = []\n",
        "\n",
        "        ANCHOR_TEXT = \"Focus on learning this:\"\n",
        "\n",
        "        i = 0\n",
        "        while i < len(lines):\n",
        "            block_lines = lines[i:i + self.lines_per_sample]\n",
        "            num_lines_used = self.lines_per_sample\n",
        "\n",
        "            block = ANCHOR_TEXT + ' [LINEBREAK] ' + ' [LINEBREAK] '.join(block_lines)\n",
        "            num_words = len(block.split())\n",
        "\n",
        "            # Ensure minimum words\n",
        "            for j in range(i + self.lines_per_sample, len(lines)):\n",
        "                if num_words >= self.min_words_per_sample:\n",
        "                    break\n",
        "                block_lines.append(lines[j])\n",
        "                num_lines_used += 1\n",
        "                block = ANCHOR_TEXT + ' [LINEBREAK] ' + ' [LINEBREAK] '.join(block_lines)\n",
        "                num_words = len(block.split())\n",
        "\n",
        "            # Avoid too many words, by cutting down blocks\n",
        "            if num_words > self.max_words_per_sample:\n",
        "                words = block.split()\n",
        "                words = words[:self.max_words_per_sample]\n",
        "                block = ' '.join(words)\n",
        "\n",
        "            # Tokenize and truncate if necessary\n",
        "            tokens = tokenizer.encode(block, truncation=True, max_length=self.max_length)\n",
        "            if len(tokens) > 1:\n",
        "                self.samples.append((tokens[:-1], tokens[1:]))\n",
        "\n",
        "            i += num_lines_used\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_ids, labels = self.samples[idx]\n",
        "\n",
        "        # Debug printing (randomly in every 1000 samples)\n",
        "        if self.debug_samples:\n",
        "            if np.random.random() < 0.001:\n",
        "                decoded = self.tokenizer.decode(input_ids, skip_special_tokens=True)\n",
        "                print(f\"[DEBUG Sample {idx}] {decoded[:100]} ...\")\n",
        "\n",
        "        return torch.tensor(input_ids, dtype=torch.long), torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "def gpt2_collate_fn(batch):\n",
        "    input_ids, labels = zip(*batch)\n",
        "    input_ids_padded = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "    labels_padded = pad_sequence(labels, batch_first=True, padding_value=0)\n",
        "    return input_ids_padded, labels_padded"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mx4E3ZbwdkeM"
      },
      "source": [
        "### 3. Dataset, Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-29T09:31:53.913304Z",
          "iopub.status.busy": "2025-04-29T09:31:53.913012Z",
          "iopub.status.idle": "2025-04-29T09:32:10.937152Z",
          "shell.execute_reply": "2025-04-29T09:32:10.936424Z",
          "shell.execute_reply.started": "2025-04-29T09:31:53.913274Z"
        },
        "id": "nIvLDP8F8r-I",
        "outputId": "7302d4d0-f4b3-4f7e-e6e6-05947e6cc7c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "26910 6734\n"
          ]
        }
      ],
      "source": [
        "train_lines, valid_lines = train_test_split(raw_lines, test_size=0.2, random_state=42)\n",
        "\n",
        "train_dataset = GPT2TokenDataset(lines=train_lines,\n",
        "                                 tokenizer=tokenizer,\n",
        "                                 lines_per_sample=config.lines,\n",
        "                                 max_length=config.max_length,\n",
        "                                 min_words_per_sample=config.min_words_per_sample,\n",
        "                                 max_words_per_sample=config.max_words_per_sample,\n",
        "                                 debug_samples=False)\n",
        "valid_dataset = GPT2TokenDataset(lines=valid_lines,\n",
        "                                 tokenizer=tokenizer,\n",
        "                                 lines_per_sample=config.lines,\n",
        "                                 max_length=config.max_length,\n",
        "                                 min_words_per_sample=config.min_words_per_sample,\n",
        "                                 max_words_per_sample=config.max_words_per_sample)\n",
        "print(len(train_dataset), len(valid_dataset))\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, collate_fn=gpt2_collate_fn)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=config.batch_size, shuffle=False, collate_fn=gpt2_collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-29T09:32:10.938207Z",
          "iopub.status.busy": "2025-04-29T09:32:10.937961Z",
          "iopub.status.idle": "2025-04-29T09:32:10.946923Z",
          "shell.execute_reply": "2025-04-29T09:32:10.946209Z",
          "shell.execute_reply.started": "2025-04-29T09:32:10.938185Z"
        },
        "id": "htK_yc0A8r-I",
        "outputId": "322c0655-a17e-4245-cd4c-532473585cc2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([16, 74]), torch.Size([16, 74]))"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "oneb = next(iter(train_loader))\n",
        "ids, lbs = oneb\n",
        "ids.shape, lbs.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYrszPxp8r-I"
      },
      "source": [
        "### 4. Create the LSTM based model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-29T09:32:10.948156Z",
          "iopub.status.busy": "2025-04-29T09:32:10.947830Z",
          "iopub.status.idle": "2025-04-29T09:32:10.961581Z",
          "shell.execute_reply": "2025-04-29T09:32:10.960668Z",
          "shell.execute_reply.started": "2025-04-29T09:32:10.948125Z"
        },
        "id": "-F2BWwMa8r-I"
      },
      "outputs": [],
      "source": [
        "class LSTMWithGPT2(nn.Module):\n",
        "    def __init__(self, tokenizer, hidden_size=256, num_layers=2, dropout=0.2):\n",
        "        super().__init__()\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.vocab_size = len(tokenizer)\n",
        "\n",
        "        # Load GPT2 embeddings\n",
        "        gpt2_model = GPT2Model.from_pretrained(\"gpt2\")\n",
        "        self.embedding = nn.Embedding(self.vocab_size, gpt2_model.wte.embedding_dim)\n",
        "\n",
        "        # Copy pretrained weights for shared vocab part\n",
        "        with torch.no_grad():\n",
        "            self.embedding.weight[:gpt2_model.wte.num_embeddings] = gpt2_model.wte.weight\n",
        "\n",
        "        self.embedding.requires_grad_(False) # Freeze embeddings\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=self.embedding.embedding_dim,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(hidden_size, self.vocab_size)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        embedded = self.embedding(input_ids)\n",
        "        lstm_out, hidden = self.lstm(embedded)\n",
        "        logits = self.fc(lstm_out)\n",
        "        return logits, hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-29T09:32:10.964465Z",
          "iopub.status.busy": "2025-04-29T09:32:10.964232Z",
          "iopub.status.idle": "2025-04-29T09:32:11.964044Z",
          "shell.execute_reply": "2025-04-29T09:32:11.963300Z",
          "shell.execute_reply.started": "2025-04-29T09:32:10.964446Z"
        },
        "id": "PbpjeS0Y8r-I",
        "outputId": "ffd2ebe1-5c3c-424a-f7e5-26c4fbccc2d5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "============================================================================================================================================\n",
              "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #                   Trainable\n",
              "============================================================================================================================================\n",
              "LSTMWithGPT2                             [16, 128]                 [16, 128, 50259]          --                        Partial\n",
              "├─Embedding: 1-1                         [16, 128]                 [16, 128, 768]            (38,598,912)              False\n",
              "├─LSTM: 1-2                              [16, 128, 768]            [16, 128, 128]            591,872                   True\n",
              "├─Linear: 1-3                            [16, 128, 128]            [16, 128, 50259]          6,483,411                 True\n",
              "============================================================================================================================================\n",
              "Total params: 45,674,195\n",
              "Trainable params: 7,075,283\n",
              "Non-trainable params: 38,598,912\n",
              "Total mult-adds (G): 1.93\n",
              "============================================================================================================================================\n",
              "Input size (MB): 0.02\n",
              "Forward/backward pass size (MB): 838.12\n",
              "Params size (MB): 182.70\n",
              "Estimated Total Size (MB): 1020.84\n",
              "============================================================================================================================================"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "model = LSTMWithGPT2(tokenizer=tokenizer, hidden_size=config.lstm_hidden_dim, num_layers=config.num_layers, dropout=config.dropout).cuda()\n",
        "\n",
        "input_ids = torch.randint(0, 1000, (16, 128), dtype=torch.long).cuda()\n",
        "summary(model, input_data=input_ids, col_names=['input_size', 'output_size', 'num_params', 'trainable'], device='cuda')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UkfBssYJYLF"
      },
      "source": [
        "### 5. Training function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-29T09:32:11.965382Z",
          "iopub.status.busy": "2025-04-29T09:32:11.965079Z",
          "iopub.status.idle": "2025-04-29T09:32:11.970127Z",
          "shell.execute_reply": "2025-04-29T09:32:11.969316Z",
          "shell.execute_reply.started": "2025-04-29T09:32:11.965357Z"
        },
        "id": "kzEI3UMp8r-I"
      },
      "outputs": [],
      "source": [
        "class HybridLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.7, gamma=2.0):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.ce = nn.CrossEntropyLoss(ignore_index=0) # ignore padding tokens\n",
        "        self.fl = kornia.losses.FocalLoss(alpha=1.0, gamma=self.gamma, reduction='mean')\n",
        "\n",
        "    def forward(self, preds, targets):\n",
        "        ce_loss = self.ce(preds, targets)\n",
        "        fl_loss = self.fl(preds, targets)\n",
        "        return self.alpha * ce_loss + (1 - self.alpha) * fl_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-29T09:32:11.971298Z",
          "iopub.status.busy": "2025-04-29T09:32:11.971004Z",
          "iopub.status.idle": "2025-04-29T09:32:11.990984Z",
          "shell.execute_reply": "2025-04-29T09:32:11.990416Z",
          "shell.execute_reply.started": "2025-04-29T09:32:11.971267Z"
        },
        "id": "HKrEuMeRwhVz"
      },
      "outputs": [],
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "def gradient_logging(model):\n",
        "    \"\"\" Logging gradients to check training stability. \"\"\"\n",
        "    total_norm = 0\n",
        "    for p in model.parameters():  # Loop over all parameters\n",
        "        if p.grad is not None:  # Only if this parameter received a gradient\n",
        "            param_norm = p.grad.data.norm(2)  # Compute L2 norm (Euclidean) of gradient\n",
        "            total_norm += param_norm.item() ** 2  # Accumulate squared norms\n",
        "    total_norm = total_norm ** 0.5  # Take the square root to get final L2 norm\n",
        "    return total_norm\n",
        "\n",
        "def bleu_scoring(target_batch, output):\n",
        "    \"\"\" Evaluation metric to measure quality of text. Output is always a number between 0 and 1, which\n",
        "    indicates how similar the candidate text is to the reference texts.\"\"\"\n",
        "    # Convert tensor outputs to list of integers\n",
        "    target_seq = target_batch.view(-1).tolist()\n",
        "    pred_seq = torch.argmax(output, dim=-1).view(-1).tolist()\n",
        "    smooth = SmoothingFunction().method1\n",
        "    # Wrap target in a list to indicate its a single reference\n",
        "    bleu_score = sentence_bleu([target_seq], pred_seq, smoothing_function=smooth)\n",
        "    return bleu_score\n",
        "\n",
        "def train_language_model(model, train_loader, valid_loader, epochs=10, lr=0.001, wd=0.001, early_stopping=2):\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
        "    loss_fn = HybridLoss(alpha=0.8, gamma=2.0)\n",
        "    # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=1)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=1, eta_min=1e-6)\n",
        "\n",
        "    early_stopping_counter = 0\n",
        "    prev_val_loss = np.Inf\n",
        "\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "\n",
        "        # Training step\n",
        "        model.train()\n",
        "        train_loss_list = []\n",
        "        for i, (input_batch, target_batch) in enumerate(train_loader):\n",
        "            # print(f\"[Batch {i}] input_batch: {input_batch.shape}, target_batch: {target_batch.shape}\")\n",
        "            input_batch, target_batch = input_batch.cuda(), target_batch.cuda()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output, _ = model(input_batch) # output: [B, seq_len, vocab_size]\n",
        "            output = output.view(-1, output.size(-1))  # flatten to [B * seq_len, vocab_size]\n",
        "            target_batch = target_batch.view(-1)  # flatten to [B * seq_len]\n",
        "\n",
        "            loss = loss_fn(output, target_batch)\n",
        "            loss.backward()\n",
        "            train_loss_list.append(loss.item())\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) # normalizes the exploding gradients\n",
        "            print(f\"Grad norm: {gradient_logging(model):.4f}\") if i % 200 == 0 else None\n",
        "\n",
        "            optimizer.step()\n",
        "            if isinstance(scheduler, torch.optim.lr_scheduler.CosineAnnealingWarmRestarts):\n",
        "                scheduler.step(epoch + i/len(train_loader)) # batchwise steps required\n",
        "\n",
        "        # Validation step\n",
        "        model.eval()\n",
        "        valid_loss_list = []\n",
        "        bleu_scores = []\n",
        "        with torch.no_grad():\n",
        "            for input_batch, target_batch in valid_loader:\n",
        "                input_batch, target_batch = input_batch.cuda(), target_batch.cuda()\n",
        "                output, _ = model(input_batch)\n",
        "                output = output.view(-1, output.size(-1))\n",
        "                target_batch = target_batch.view(-1)\n",
        "                loss = loss_fn(output, target_batch)\n",
        "                bleu_scores.append(bleu_scoring(target_batch, output))\n",
        "                valid_loss_list.append(loss.item())\n",
        "\n",
        "        avg_bleu_score = np.mean(bleu_scores)\n",
        "        avg_valid_loss = np.mean(valid_loss_list)\n",
        "        avg_train_loss = np.mean(train_loss_list)\n",
        "        gc.collect()\n",
        "\n",
        "        # Handle early stopping condition\n",
        "        if epoch == 0 or avg_valid_loss < prev_val_loss:\n",
        "            torch.save(model.state_dict(), './best_state.pt')\n",
        "            early_stopping_counter = 0\n",
        "            prev_val_loss = avg_valid_loss\n",
        "        else:\n",
        "            early_stopping_counter += 1\n",
        "            if early_stopping_counter >= early_stopping:\n",
        "                print(\"Early stopping triggered.\")\n",
        "                break\n",
        "\n",
        "        # Report and step scheduler\n",
        "        print(f\"\"\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Valid Loss: {avg_valid_loss:.4f}, Bleu Score: {avg_bleu_score:.4f}\"\"\")\n",
        "        print(f\"Learning rate: {optimizer.param_groups[0]['lr']}\")\n",
        "        if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
        "            scheduler.step(avg_valid_loss)\n",
        "\n",
        "    model.load_state_dict(torch.load('./best_state.pt'))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-29T09:32:11.991902Z",
          "iopub.status.busy": "2025-04-29T09:32:11.991640Z",
          "iopub.status.idle": "2025-04-29T10:10:25.930156Z",
          "shell.execute_reply": "2025-04-29T10:10:25.929198Z",
          "shell.execute_reply.started": "2025-04-29T09:32:11.991882Z"
        },
        "id": "G9nVlQtyfJ6b",
        "outputId": "2f733f0a-f1dd-4790-b05d-76981c85f570"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/20 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Grad norm: 0.1679\n",
            "Grad norm: 0.3150\n",
            "Grad norm: 0.3555\n",
            "Grad norm: 0.5158\n",
            "Grad norm: 0.3002\n",
            "Grad norm: 0.2667\n",
            "Grad norm: 0.1360\n",
            "Grad norm: 0.1271\n",
            "Grad norm: 0.1041\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  5%|▌         | 1/20 [02:44<52:06, 164.56s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20, Train Loss: 3.5400, Valid Loss: 3.0225, Bleu Score: 0.1774\n",
            "Learning rate: 0.009046277920252426\n",
            "Grad norm: 0.1582\n",
            "Grad norm: 0.1214\n",
            "Grad norm: 0.1046\n",
            "Grad norm: 0.1073\n",
            "Grad norm: 0.1049\n",
            "Grad norm: 0.1312\n",
            "Grad norm: 0.1175\n",
            "Grad norm: 0.1098\n",
            "Grad norm: 0.0997\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 10%|█         | 2/20 [05:28<49:19, 164.44s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/20, Train Loss: 2.9008, Valid Loss: 2.8036, Bleu Score: 0.1888\n",
            "Learning rate: 0.006547206534724658\n",
            "Grad norm: 0.1029\n",
            "Grad norm: 0.1078\n",
            "Grad norm: 0.1069\n",
            "Grad norm: 0.1097\n",
            "Grad norm: 0.1033\n",
            "Grad norm: 0.1041\n",
            "Grad norm: 0.0958\n",
            "Grad norm: 0.1121\n",
            "Grad norm: 0.1017\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 15%|█▌        | 3/20 [08:13<46:34, 164.39s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/20, Train Loss: 2.7513, Valid Loss: 2.7339, Bleu Score: 0.1924\n",
            "Learning rate: 0.003457345823553637\n",
            "Grad norm: 0.1465\n",
            "Grad norm: 0.1060\n",
            "Grad norm: 0.1087\n",
            "Grad norm: 0.1065\n",
            "Grad norm: 0.1895\n",
            "Grad norm: 0.1079\n",
            "Grad norm: 0.0985\n",
            "Grad norm: 0.1040\n",
            "Grad norm: 0.1037\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 4/20 [10:56<43:44, 164.05s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/20, Train Loss: 2.6716, Valid Loss: 2.7020, Bleu Score: 0.1910\n",
            "Learning rate: 0.0009569175579037744\n",
            "Grad norm: 0.1180\n",
            "Grad norm: 0.1128\n",
            "Grad norm: 0.1018\n",
            "Grad norm: 0.1301\n",
            "Grad norm: 0.1015\n",
            "Grad norm: 0.1599\n",
            "Grad norm: 0.1120\n",
            "Grad norm: 0.1090\n",
            "Grad norm: 0.1192\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 25%|██▌       | 5/20 [13:40<40:59, 163.96s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/20, Train Loss: 2.6305, Valid Loss: 2.6968, Bleu Score: 0.1920\n",
            "Learning rate: 1.000348822367915e-06\n",
            "Grad norm: 0.1348\n",
            "Grad norm: 0.1115\n",
            "Grad norm: 0.1456\n",
            "Grad norm: 0.1065\n",
            "Grad norm: 0.1218\n",
            "Grad norm: 0.0963\n",
            "Grad norm: 0.1059\n",
            "Grad norm: 0.1047\n",
            "Grad norm: 0.1259\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 30%|███       | 6/20 [16:23<38:12, 163.77s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/20, Train Loss: 2.6947, Valid Loss: 2.7167, Bleu Score: 0.1919\n",
            "Learning rate: 0.009046277920252426\n",
            "Grad norm: 0.1013\n",
            "Grad norm: 0.1052\n",
            "Grad norm: 0.0984\n",
            "Grad norm: 0.1284\n",
            "Grad norm: 0.1239\n",
            "Grad norm: 0.1037\n",
            "Grad norm: 0.1129\n",
            "Grad norm: 0.0971\n",
            "Grad norm: 0.0981\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 35%|███▌      | 7/20 [19:07<35:29, 163.84s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7/20, Train Loss: 2.6282, Valid Loss: 2.6925, Bleu Score: 0.1940\n",
            "Learning rate: 0.006547206534724657\n",
            "Grad norm: 0.1097\n",
            "Grad norm: 0.1927\n",
            "Grad norm: 0.0990\n",
            "Grad norm: 0.1112\n",
            "Grad norm: 0.1053\n",
            "Grad norm: 0.0988\n",
            "Grad norm: 0.0980\n",
            "Grad norm: 0.1735\n",
            "Grad norm: 0.1080\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|████      | 8/20 [21:52<32:47, 163.92s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8/20, Train Loss: 2.5668, Valid Loss: 2.6768, Bleu Score: 0.1933\n",
            "Learning rate: 0.003457345823553637\n",
            "Grad norm: 0.0988\n",
            "Grad norm: 0.0975\n",
            "Grad norm: 0.1109\n",
            "Grad norm: 0.1252\n",
            "Grad norm: 0.0939\n",
            "Grad norm: 0.1113\n",
            "Grad norm: 0.1033\n",
            "Grad norm: 0.1084\n",
            "Grad norm: 0.1155\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 45%|████▌     | 9/20 [24:35<30:02, 163.89s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9/20, Train Loss: 2.5142, Valid Loss: 2.6685, Bleu Score: 0.1956\n",
            "Learning rate: 0.0009569175579037761\n",
            "Grad norm: 0.1003\n",
            "Grad norm: 0.1032\n",
            "Grad norm: 0.1111\n",
            "Grad norm: 0.1091\n",
            "Grad norm: 0.1038\n",
            "Grad norm: 0.0999\n",
            "Grad norm: 0.0981\n",
            "Grad norm: 0.1386\n",
            "Grad norm: 0.1061\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 10/20 [27:19<27:19, 163.91s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/20, Train Loss: 2.4813, Valid Loss: 2.6671, Bleu Score: 0.1964\n",
            "Learning rate: 1.000348822367915e-06\n",
            "Grad norm: 0.1062\n",
            "Grad norm: 0.1814\n",
            "Grad norm: 0.1165\n",
            "Grad norm: 0.1216\n",
            "Grad norm: 0.1168\n",
            "Grad norm: 0.1080\n",
            "Grad norm: 0.1105\n",
            "Grad norm: 0.1148\n",
            "Grad norm: 0.1117\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 55%|█████▌    | 11/20 [30:02<24:32, 163.63s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11/20, Train Loss: 2.5758, Valid Loss: 2.7101, Bleu Score: 0.1909\n",
            "Learning rate: 0.009046277920252428\n",
            "Grad norm: 0.1182\n",
            "Grad norm: 0.0981\n",
            "Grad norm: 0.1154\n",
            "Grad norm: 0.1130\n",
            "Grad norm: 0.1015\n",
            "Grad norm: 0.1029\n",
            "Grad norm: 0.1067\n",
            "Grad norm: 0.1215\n",
            "Grad norm: 0.0998\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|██████    | 12/20 [32:46<21:49, 163.70s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12/20, Train Loss: 2.5376, Valid Loss: 2.7045, Bleu Score: 0.1952\n",
            "Learning rate: 0.0065472065347246585\n",
            "Grad norm: 0.1099\n",
            "Grad norm: 0.1042\n",
            "Grad norm: 0.0978\n",
            "Grad norm: 0.1137\n",
            "Grad norm: 0.1101\n",
            "Grad norm: 0.1080\n",
            "Grad norm: 0.1296\n",
            "Grad norm: 0.1017\n",
            "Grad norm: 0.1078\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 65%|██████▌   | 13/20 [35:30<19:05, 163.71s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13/20, Train Loss: 2.4886, Valid Loss: 2.6987, Bleu Score: 0.1975\n",
            "Learning rate: 0.003457345823553638\n",
            "Grad norm: 0.0992\n",
            "Grad norm: 0.1141\n",
            "Grad norm: 0.0977\n",
            "Grad norm: 0.1005\n",
            "Grad norm: 0.1035\n",
            "Grad norm: 0.1093\n",
            "Grad norm: 0.1129\n",
            "Grad norm: 0.0990\n",
            "Grad norm: 0.0950\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 65%|██████▌   | 13/20 [38:13<20:35, 176.44s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Early stopping triggered.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "<ipython-input-54-16cc7d3e11c6>:94: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('./best_state.pt'))\n"
          ]
        }
      ],
      "source": [
        "trained_model = train_language_model(model, train_loader, valid_loader, epochs=config.epochs, lr=config.lr, wd=config.wd, early_stopping=config.early_stopping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-29T10:10:25.931459Z",
          "iopub.status.busy": "2025-04-29T10:10:25.931178Z",
          "iopub.status.idle": "2025-04-29T10:10:26.337874Z",
          "shell.execute_reply": "2025-04-29T10:10:26.337155Z",
          "shell.execute_reply.started": "2025-04-29T10:10:25.931435Z"
        },
        "id": "T-UW6fUE8r-I",
        "outputId": "2a02abae-f437-4b22-ffe4-668d6d8bc0b4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-29T10:10:26.338870Z",
          "iopub.status.busy": "2025-04-29T10:10:26.338629Z",
          "iopub.status.idle": "2025-04-29T10:10:27.517816Z",
          "shell.execute_reply": "2025-04-29T10:10:27.516960Z",
          "shell.execute_reply.started": "2025-04-29T10:10:26.338836Z"
        },
        "id": "ulhEZ15f8r-I",
        "outputId": "6960f960-bbef-4d64-e4c2-e6127e5919be"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-57-5b014d97c841>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  inference_model.load_state_dict(torch.load('./best_state.pt'))\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "LSTMWithGPT2(\n",
              "  (embedding): Embedding(50259, 768)\n",
              "  (lstm): LSTM(768, 128, num_layers=2, batch_first=True, dropout=0.3)\n",
              "  (fc): Linear(in_features=128, out_features=50259, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create inference model\n",
        "inference_model = LSTMWithGPT2(tokenizer=tokenizer, hidden_size=config.lstm_hidden_dim, num_layers=config.num_layers, dropout=config.dropout).cuda()\n",
        "inference_model.load_state_dict(torch.load('./best_state.pt'))\n",
        "inference_model.to(device=\"cuda\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a4nzDy2JdPA"
      },
      "source": [
        "### 5. Text generation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-29T10:40:36.717811Z",
          "iopub.status.busy": "2025-04-29T10:40:36.717489Z",
          "iopub.status.idle": "2025-04-29T10:40:36.725360Z",
          "shell.execute_reply": "2025-04-29T10:40:36.724453Z",
          "shell.execute_reply.started": "2025-04-29T10:40:36.717786Z"
        },
        "id": "7GkDVrYp8r-I"
      },
      "outputs": [],
      "source": [
        "def restore_text(decoded_text):\n",
        "    \"\"\"Restore the text after decoding.\"\"\"\n",
        "    text = decoded_text.replace(' [LINEBREAK] ', '\\n').replace('[LINEBREAK]', '\\n')\n",
        "    text = text.replace(' [STANZABREAK] ', '\\n\\n').replace('[STANZABREAK]', '\\n\\n')\n",
        "    return text.strip()\n",
        "\n",
        "def generate_text(model, tokenizer, seed_text, max_length=50, top_k=10, temperature=1.0, debug=False, check_word=\"\"):\n",
        "    \"\"\"\n",
        "    Generate text given a seed_text using the trained model and tokenizer.\n",
        "\n",
        "    Parameters:\n",
        "    - model: The trained LSTM model to generate text from.\n",
        "    - tokenizer: The GPT-2 tokenizer.\n",
        "    - seed_text: The initial text to start generating from.\n",
        "    - max_length: The maximum length of the generated sequence.\n",
        "    - top_k: Only consider the top k most likely next tokens.\n",
        "    - temperature: Controls randomness in sampling. Higher values make it more random.\n",
        "    - debug: Print debug information.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenize with the GPT-2 tokenizer\n",
        "    tokens_encoded = tokenizer.encode(seed_text, return_tensors='pt').cuda()  # [1, seq_len]\n",
        "    generated = tokens_encoded.squeeze().tolist()  # Convert to list for easier handling\n",
        "\n",
        "    if debug:\n",
        "        print(\"Vocab size:\", tokenizer.vocab_size)\n",
        "        print(f\"'{check_word}' in vocab?\", check_word in tokenizer.get_vocab())\n",
        "        print(\"Encoded:\", tokens_encoded)\n",
        "        dec = tokenizer.decode(tokens_encoded[0])\n",
        "        print(\"Decoded:\", dec); print()\n",
        "\n",
        "    hidden = None  # For LSTM we need to reset hidden state for each generation cycle\n",
        "\n",
        "    # Generate tokens one at a time\n",
        "    for _ in range(max_length):\n",
        "        output, hidden = model(tokens_encoded)  # [B, seq_len, vocab_size]\n",
        "        logits = output[0, -1] / temperature\n",
        "\n",
        "        # Apply top-k sampling\n",
        "        # (random sampling from the top k tokens with highest probabilities)\n",
        "        top_k_logits, top_k_indices = torch.topk(logits, k=top_k)\n",
        "\n",
        "        # Sample from the top-k tokens using softmax\n",
        "        probs = torch.softmax(top_k_logits, dim=-1)\n",
        "        next_token_idx = top_k_indices[torch.multinomial(probs, 1)]\n",
        "\n",
        "        # Add the predicted token to the sequence\n",
        "        generated.append(next_token_idx.item())\n",
        "\n",
        "        # If the special token for stanza break is found, stop early\n",
        "        if next_token_idx.item() == tokenizer.encode(\"[STANZABREAK]\")[0]:\n",
        "            break\n",
        "\n",
        "        # Update input sequence by appending the predicted token for next step\n",
        "        tokens_encoded = torch.cat([tokens_encoded, next_token_idx.unsqueeze(0)], dim=1)\n",
        "\n",
        "    # Decode generated token sequence back into words\n",
        "    generated_tokens = tokenizer.decode(generated)\n",
        "\n",
        "    # Restore the text with special tokens\n",
        "    generated_output = restore_text(generated_tokens)\n",
        "\n",
        "    return generated_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-29T10:40:39.718300Z",
          "iopub.status.busy": "2025-04-29T10:40:39.717982Z",
          "iopub.status.idle": "2025-04-29T10:40:40.904395Z",
          "shell.execute_reply": "2025-04-29T10:40:40.903493Z",
          "shell.execute_reply.started": "2025-04-29T10:40:39.718270Z"
        },
        "id": "eU9c5X3Q8r-I",
        "outputId": "c43e90d9-f080-40e8-8afe-f508a0517443"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "************************************************** \n",
            "Test generation with temperature (0.7):\n",
            " **************************************************\n",
            "as all merchant says:\n",
            "as all merchant says \n",
            " \n",
            " to make it so. \n",
            " \n",
            " and, by his love, but not for the queen's life, \n",
            " \n",
            " and now, my good lord, and I am sent to do; \n",
            " \n",
            " with him, and I do not be not, for ...\n",
            "\n",
            "come with me my dear:\n",
            "come with me my dear. \n",
            " \n",
            " and in the sea, and my love's son, \n",
            " \n",
            " to see a little man, and so, and I, \n",
            " \n",
            " LUCENTIO. I am a king, I pray you, and a good lady, \n",
            " \n",
            " and ...\n",
            "\n",
            "let the feast begin:\n",
            "let the feast begin \n",
            " \n",
            " and the king's name, and I'll have it; therefore, sir, \n",
            " \n",
            " I would not be my friend. ...\n",
            "\n",
            "bring your dancing shoes and:\n",
            "bring your dancing shoes and \n",
            " \n",
            " to be the best of the world, that I have seen \n",
            " \n",
            " AENEAS. what's she? \n",
            " \n",
            " and in the king of the court. \n",
            " \n",
            " that he will not be to be so-favour'd ...\n",
            "\n",
            "don't mind the bad roumors, it:\n",
            "don't mind the bad roumors, it is \n",
            " \n",
            " and yet so you have heard, and in our eyes; \n",
            " \n",
            " and now the world is to see me to be. \n",
            " \n",
            " and I should make you, good lord, \n",
            " \n",
            " and then I am a good woman's death; ' ...\n",
            "\n",
            "************************************************** \n",
            "Test generation with temperature (1.0):\n",
            " **************************************************\n",
            "as all merchant says:\n",
            "as all merchant says \n",
            " \n",
            " BAPTISTA. he's not so much a man \n",
            " \n",
            " as that the day's eyes are out of me, \n",
            " \n",
            " ALCONY. what's thy name? \n",
            " \n",
            " for thou shalt have no stomach; for if I ...\n",
            "\n",
            "come with me my dear:\n",
            "come with me my dear. \n",
            " \n",
            " to hear me be not well of me of this-skin, \n",
            " \n",
            " that I am bound from my head to-night? \n",
            " \n",
            " and, as you do not be a good man, to see me. \n",
            " \n",
            " to know you to ...\n",
            "\n",
            "let the feast begin:\n",
            "let the feast begin. \n",
            " \n",
            " to th' field-like, and a fairman, \n",
            " \n",
            " and so much as I had in love that you may. \n",
            " \n",
            " for your sake that the duke be thou so-favour'd \n",
            " \n",
            " I will to be to ...\n",
            "\n",
            "bring your dancing shoes and:\n",
            "bring your dancing shoes and \n",
            " \n",
            " LAFEU. what, is it not the world? \n",
            " \n",
            " the very army is gone for your majesty, \n",
            " \n",
            " the other of the night, and the prince's wife, \n",
            " \n",
            " and therefore, with the best and most valiant ...\n",
            "\n",
            "don't mind the bad roumors, it:\n",
            "don't mind the bad roumors, it \n",
            " \n",
            " and so, I'll go for him; but in her life. \n",
            " \n",
            " and that the king is to-morrow with ajax. A \n",
            " \n",
            " AUSTRIA. he is no man- \n",
            " \n",
            " BUSHY. what is ...\n",
            "\n",
            "************************************************** \n",
            "Test generation with temperature (1.3):\n",
            " **************************************************\n",
            "as all merchant says:\n",
            "as all merchant says. ...\n",
            "\n",
            "come with me my dear:\n",
            "come with me my dear father in a \n",
            " \n",
            " in that thou mayst know him with our hearts. \n",
            " \n",
            " to make him not for the king, I am glad. \n",
            " \n",
            " with any man in our hands of our lives. \n",
            " \n",
            " to be my heart is as in my ...\n",
            "\n",
            "let the feast begin:\n",
            "let the feast begin \n",
            " \n",
            " but I am in my father's death is so late. \n",
            " \n",
            " KING. what should you see a man; and this he said not? \n",
            " \n",
            " the queen doth, as it shall be no more \n",
            " \n",
            " that thou hast done to me ...\n",
            "\n",
            "bring your dancing shoes and:\n",
            "bring your dancing shoes and his \n",
            " \n",
            " and now to give me no, I have done so \n",
            " \n",
            " that she hath done. \n",
            " \n",
            " to have you with such another of all your eyes, \n",
            " \n",
            " I'll bethink the devil to him to-morrow \n",
            " \n",
            " that ...\n",
            "\n",
            "don't mind the bad roumors, it:\n",
            "don't mind the bad roumors, it is \n",
            " \n",
            " that I have been to thee, and so much? \n",
            " \n",
            " and he's a fool for you. what would I do not \n",
            " \n",
            " for thou art an ounce of our royal self- \n",
            " \n",
            " to you that I would know my father's ...\n",
            "\n",
            "************************************************** \n",
            "Test generation with temperature (2.0):\n",
            " **************************************************\n",
            "as all merchant says:\n",
            "as all merchant says the world's \n",
            " \n",
            " I cannot not be admitted. \n",
            " \n",
            " for the man-cathby, a man to thurio the day in the world with the \n",
            " \n",
            " for the man-and what it did the world be ready as \n",
            " \n",
            " I ...\n",
            "\n",
            "come with me my dear:\n",
            "come with me my dear lord john? \n",
            " \n",
            " the other that he were but I will, \n",
            " \n",
            " with this. ...\n",
            "\n",
            "let the feast begin:\n",
            "let the feast begin in my \n",
            " you do no further, as to be so bold, the son of \n",
            " \n",
            " in your own pleasure-bed at his life. he was a good \n",
            " \n",
            " the man is not, if you can do this ring-garter. the news that hath ...\n",
            "\n",
            "bring your dancing shoes and:\n",
            "bring your dancing shoes and, but he is. ...\n",
            "\n",
            "don't mind the bad roumors, it:\n",
            "don't mind the bad roumors, it be; so much he \n",
            " \n",
            " I am sorry; he will, to-and-twig.' if you \n",
            " \n",
            " in my heart. ...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Generate text\n",
        "ideas = [\"as all merchant says\",\n",
        "         \"come with me my dear\",\n",
        "         \"let the feast begin\",\n",
        "         \"bring your dancing shoes and\",\n",
        "         \"don't mind the bad roumors, it\"\n",
        "         ]\n",
        "\n",
        "max_l = 60 # length of the generated text\n",
        "top_k = 15 # range of tokens to choose from\n",
        "temperature = [0.7, 1.0, 1.3, 2.0] # randomness factor\n",
        "debug = False\n",
        "check_word = \"\"\n",
        "\n",
        "# Testing generation\n",
        "for i, t in enumerate(temperature):\n",
        "    print(\"*\"*50, f\"\\nTest generation with temperature ({t}):\\n\",\"*\"*50)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for text in ideas:\n",
        "            output = generate_text(\n",
        "                inference_model,\n",
        "                tokenizer,\n",
        "                seed_text=text,\n",
        "                max_length=max_l,\n",
        "                top_k=top_k,\n",
        "                temperature=t,\n",
        "                debug=debug,\n",
        "                check_word=check_word\n",
        "            )\n",
        "            print(f\"{text}:\\n{output} ...\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-29T10:52:52.964014Z",
          "iopub.status.busy": "2025-04-29T10:52:52.963705Z",
          "iopub.status.idle": "2025-04-29T10:52:52.967845Z",
          "shell.execute_reply": "2025-04-29T10:52:52.967012Z",
          "shell.execute_reply.started": "2025-04-29T10:52:52.963990Z"
        },
        "id": "B0WfAnslJQ6A"
      },
      "outputs": [],
      "source": [
        "## Poem generated by ChatGPT:\n",
        "\n",
        "# 1.\n",
        "# \"as all merchant says, in whispers low,\n",
        "\n",
        "# The tides of fortune shift like winds that blow,\n",
        "\n",
        "# Yet trust in love, for it shall ever grow.\"\n",
        "\n",
        "# 2.\n",
        "# \"come with me, my dear, beneath the moon's soft light,\n",
        "\n",
        "# Where dreams entwine and hearts take flight,\n",
        "\n",
        "# In this enchanted realm, all wrongs feel right.\"\n",
        "\n",
        "# 3.\n",
        "# \"let the feast begin, with laughter's sweet embrace,\n",
        "\n",
        "# As joy and mirth adorn this hallowed space,\n",
        "\n",
        "# With every toast, we celebrate our grace.\"\n",
        "\n",
        "# 4.\n",
        "# \"bring your dancing shoes and let your spirit soar,\n",
        "\n",
        "# For in this revelry, we shall seek no more,\n",
        "\n",
        "# Each step a story, each twirl a lore.\"\n",
        "\n",
        "# 5.\n",
        "# \"don't mind the bad rumors, it’s but a fleeting breeze,\n",
        "\n",
        "# For truth shall shine like stars through darkened trees,\n",
        "\n",
        "# In love's embrace, we find our hearts at ease.\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_29rejW8r-I"
      },
      "source": [
        "### Summary of the generation outputs:\n",
        "\n",
        "- top-k:\n",
        "    - 15-20 gives probably the most relevant results\n",
        "    - tried 50, it became too chaotic\n",
        "- temperature:\n",
        "    - 0.7: sometimes repetitive, but still performs well\n",
        "    - 1.0: probably the best scenario\n",
        "    - 1.3-2.0: creative, but sometimes inappropriate, random words\n",
        "- conclusions:\n",
        "    - The generated outputs are still very random in most cases (little coherence between words), most important changes could be done in the data preparation phase.\n",
        "    - The sentences end meaninglessly, even though the model knows that a linebreak will occur after 6-8 words, it does not finish it.\n",
        "    - The model learned the whole corpus with many different dialogues. To make it poetic we should only train the model on poems only.\n",
        "    - It could be also advantegous to train it with more lines per sample (4 or 5), although it slows down the process."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 715814,
          "sourceId": 1246668,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 870709,
          "sourceId": 1483651,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30918,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}